---
title: "The Rhode Island Student Growth Model"
subtitle: "A Technical Overview of the 2017-2018 Student Growth Percentile Calculations"
author:
  - name: Adam R. VanIwaarden
  - name: Damian W. Betebenner
  - name: <em>National Center for the Improvement<br></br> of Educational Assessment (NCIEA)</em>
date: October 2018
abstract: "DRAFT REPORT - DO NOT CITE! <br></br> This report provides details about the Rhode Island Student Growth Model methodology and presents a descriptive analysis of the 2018 SGP calculation process and results."
---

<!--SGPreport-->

<!-- 
This document was written by Damian Betebenner & Adam VanIwaarden for the State of Rhode Island Department of Education (RIDE).

	Original Draft:  September 3, 2018
	Final Draft:     September 5, 2018
-->


```{r, echo=FALSE, include=FALSE}
  ## set a universal Cache path
  knitr::opts_chunk$set(cache.path = "_cache/GA_SGP_2018")

  ##  Load some R packages and functions required for HTML table creation silently.  
  ##  Load SGP and other packages here to avoid messages.
  require(SGP)
	require(data.table)
	require(Gmisc)
  require(htmlTable)

  ##  Set Table, Figure and Equation Counters
  options(table_number=0)
  options("fig_caption_no"=0)
	options(fig_caption_no_sprintf = "**Figure %s:**   %s")
	options("fig_caption_no_roman"=FALSE)
	options("equation_counter" = 0)

  ###
	### Lists of RICAS and PSAT/SAT subjects.  Used to subset summary tables and data below.
	###

  subject_order <- c("ELA", "MATHEMATICS", 
										 "ELA_PSAT_10", "ELA_SAT",
										 "MATHEMATICS_PSAT_10", "MATHEMATICS_SAT")
	GL_subjects <- c("ELA", "MATHEMATICS")
	EOCT_subjects<-c("ELA_PSAT_10", "ELA_SAT",
									 "MATHEMATICS_PSAT_10", "MATHEMATICS_SAT")
```

# Introduction

This report contains details on the 2017-2018 implementation of the student growth percentiles (SGP) model for the state of Rhode Island.  The National Center for the Improvement of Educational Assessment (NCIEA) contracted with the Rhode Island Department of Education (RIDE) to implement the SGP methodology using data derived from the [Rhode Island Comprehensive Assessment System (RICAS)](http://www.ride.ri.gov/InstructionAssessment/Assessment/RICASAssessments.aspx) and [PSAT10 and the SAT assessments](http://www.ride.ri.gov/InstructionAssessment/Assessment/PSATandSAT.aspx) to create the Rhode Island student growth model.  The goal of the engagement with RIDE is to create a set of open source analytics techniques and conduct analyses that will eventually be conducted exclusively by RIDE in following years.

The SGP methodology is an open source norm- and criterion-referenced student growth analysis that produces student growth percentiles for each student in the state with adequate longitudinal data.  The methodology is currently used for many purposes.  States and districts have used the results in various ways including parent/student diagnostic reporting, institutional improvement, and school and educator accountability.  Specifics about the manner in which growth is included in school and educator accountability can be found in documents related to those accountability systems.

This report includes four sections:

- ***Data - *** includes details on the decision rules used in the raw data preparation and student record validation.
- ***Analytics - *** introduces some of the basic statistical methods and the computational process implemented in the 2018 analyses.^[More in-depth treatment of the SGP Methodology can be found [here](https://github.com/CenterForAssessment/SGP_Resources/tree/master/articles) and in Appendix B of this report.]
- ***Goodness of Fit - *** investigates how well the statistical models used to produce SGPs fit Rhode Island students' data.  This includes discussion of goodness of fit plots and the student-level correlations between SGP and prior achievement.
- ***SGP Results - *** provides basic descriptive statistics from the 2018 analyses at both the state and school levels.

This report also includes multiple appendices.  Appendix A displays Goodness of Fit plots for each analysis conducted in 2018.  Appendix B provides a technical description of the SGP Methodology.  Appendix C is an investigation of potential ceiling and/or floor effects present in the Rhode Island assessment data and growth analyses.

```{r, cache=FALSE, results="asis", echo=FALSE}
	Literasee:::pageBreak()
```



# Data

RIDE supplied RICAS and PSAT/SAT data used in the 2018 SGP analyses to the NCIEA in late summer of 2018.  These test records were added to prior-years data from the [Partnership for Assessment of Readiness for College and Careers (PARCC) consortium](https://parcc-assessment.org/) assessments to create the longitudinal data set from which the 2018 SGPs were calculated.  Subsequent years' analyses will augment this multi-year data set allowing RIDE to maintain comprehensive longitudinal data for all students taking the RICAS and PSAT/SAT assessments.

Student Growth Percentiles are produced for students that have a current score and at least one prior score in either the same subject or a related content area.  For the 2018 academic year SGPs were produced for both RICAS and PSAT/SAT in English Language Arts (ELA) and Mathematics.

## Longitudinal Data
Growth analyses on assessment data require data that are linked to individual students over time.  Student growth percentile analyses require, at a minimum two, and preferably three years of assessment data for analysis of student progress.  To this end it is necessary that a unique student identifier be available so that student data records across years can be merged with one another and subsequently examined.  Because some records in the assessment data set contain students with more than one test score in a content area in a given year, a process to create unique student records in each content area by year combination was required in order to carry out subsequent growth analyses.  Furthermore, student records may be invalidated for other reasons.  The following business rules were used to either invalidate particular student records or select the appropriate record for use in the analyses.

### General business rules

1.  Student records are invalidated if the student identifier is missing.
2.  Student records with missing ("NA") scores or scale scores outside of the possible range (usually 0) are invalidated.
3.  Student records with any administrative invalidation flag (for example, identifying test irregularities, students that did not attempt the test, or other issues) are invalidated.


### RICAS specific business rules

1.  If a student has multiple records (duplicate from the same subject, grade and administration period), their highest score was selected.
2.  Records were invalidated if a student had a `TEST_ATTEMPT` value of "P".
3.  Home schooled students' records were invalidated.

Table `r tblNum(1)` shows the number of valid RICAS student records available for analysis after applying the general and RICAS specific business rules.^[This does not represent the number of SGPs produced, however, because students are required to have at least one prior score available as well.]

```{r, cache=TRUE, echo=FALSE, include=FALSE}
	n_tbl_EOG <- table(Rhode_Island_SGP@Data[YEAR=='2017_2018' & CONTENT_AREA %in% GL_subjects & VALID_CASE=='VALID_CASE']$CONTENT_AREA, Rhode_Island_SGP@Data[YEAR=='2017_2018' & CONTENT_AREA %in% GL_subjects & VALID_CASE=='VALID_CASE']$GRADE)
```
```{r, results='asis', echo=FALSE, N_tableEOG}
	n_tbl_EOG_B <- n_tbl_EOG[match(GL_subjects, row.names(n_tbl_EOG)) ,]
	n_tbl_EOG_B <- cbind('Content Area'=sapply(GL_subjects, capwords, special.words=c("ELA", "US")), n_tbl_EOG_B)
	n_tbl_EOG_B <- prettyNum(n_tbl_EOG_B, preserve.width = "individual",big.mark=',')
	n_tbl_EOG_B[which(n_tbl_EOG_B==0)] <- ''
	row.names(n_tbl_EOG_B) <- NULL

  cat(dualTable(as.matrix(n_tbl_EOG_B), 
  	align=paste(rep('r', dim(n_tbl_EOG_B)[2]), collapse=''),
		n.cgroup=c(1, dim(n_tbl_EOG_B)[2]-1), cgroup=c("", "Grades"),
		caption='Number of Valid RICAS Student Records by Grade and Subject for 2018'))
```


### PSAT/SAT specific business rules

1.  If a student has multiple records from the same subject and administration period, their highest score was selected.
2.  Records were invalidated if a student had a (subject specific) participation flag value of "N".

Table `r tblNum(1)` shows the total number of valid PSAT/SAT student records available for analysis after applying the general and PSAT/SAT specific business rules.

```{r, cache=TRUE, echo=FALSE, include=FALSE}
	n_tbl_EOC <- table(Rhode_Island_SGP@Data[YEAR=='2017_2018' & CONTENT_AREA %in% EOCT_subjects & VALID_CASE=='VALID_CASE']$CONTENT_AREA, Rhode_Island_SGP@Data[YEAR=='2017_2018' & CONTENT_AREA %in% EOCT_subjects & VALID_CASE=='VALID_CASE']$GRADE)
```
```{r, results='asis', echo=FALSE, N_tableEOC}
	n_tbl_EOCT_B <- n_tbl_EOC[match(EOCT_subjects, row.names(n_tbl_EOC)) ,]
	n_tbl_EOCT_B <- cbind('Content Area'=sapply(EOCT_subjects, capwords, special.words=c("ELA", "PSAT", "SAT")), 'Valid Records' = n_tbl_EOCT_B)
	n_tbl_EOCT_B <- prettyNum(n_tbl_EOCT_B, preserve.width = "individual",big.mark=',')
	row.names(n_tbl_EOCT_B) <- NULL

  cat(dualTable(as.matrix(n_tbl_EOCT_B), align=paste(rep('r', dim(n_tbl_EOCT_B)[2]), collapse=''), 
		caption='Total Number of Valid PSAT/SAT Student Records by Subject for 2018'))
  
  Literasee:::pageBreak()
```


# Analytics

This section provides basic details about the calculation of student growth percentiles from Rhode Island state assessment data using the [`R` Software Environment](http://www.r-project.org/) [@Rsoftware] in conjunction with the [`SGP` package](https://github.com/CenterForAssessment/SGP) [@sgp2018].

Broadly, the SGP analysis of the Rhode Island longitudinal student assessment data takes place in two steps:

1. Data Preparation
2. Data Analysis

Those familiar with data analysis know that the bulk of the effort in the above two step process lies with Step 1: Data Preparation.  Following thorough data cleaning and preparation, data analysis using the `SGP` package takes clean data and makes it as easy as possible to calculate, summarize, output and visualize the results from SGP analyses.

## Data Preparation

The data preparation step involves taking data provided by RIDE and producing a `.Rdata` file that will subsequently be analyzed in Step 2. This process is carried out annually as new data becomes available from the state assessment program.  

The comma-separated values (CSV) files that RIDE provides were cleaned and processed by first reading them into [`R`](http://www.r-project.org/) and then slightly modifying the variables. The result is a `.Rdata` file containing data in the format suitable for analysis with the [`SGP` package](https://github.com/CenterForAssessment/SGP).  This data is combined with a subset of prior years' PARCC data necessary to complete the 2018 analyses.  With an appropriate longitudinal data set prepared, we move to the calculation of student-level SGPs.

## 2018 Data Analysis

The objective of the student growth percentile (SGP) analysis is to describe how (a)typical a student's growth is by examining his/her current achievement relative to students with a similar achievement history; i.e his/her *academic peers* (see [this presentation](https://github.com/CenterForAssessment/SGP_Resources/blob/master/presentations/Academic_Peer_Slides.pdf)). This norm-referenced growth quantity is estimated using quantile regression [@Koenker:2005] to model curvilinear functional relationships between student's prior and current scores.  One hundred such regression models are calculated for each separate analysis (defined as a unique ***year** by **content area** by **grade** by **prior order*** combination).  The end product of these 100 separate regression models is a single coefficient matrix, which serves as a look-up table to relate prior student achievement to current achievement for each percentile. This process ultimately leads to tens of thousands of model calculations during each of Rhode Island's annual batch of analyses.  For a more in-depth discussion of SGP calculation, see Betebenner [-@Betebenner:2009] and Appendix B of this report.

The 2018 Rhode Island SGP analyses follow a work flow that includes the following 4 steps:

1. Update the Rhode Island assessment meta-data required for SGP calculations using the `SGP` package.
2. Create annual SGP configurations for analyses.
3. Conduct all RICAS and PSAT/SAT SGP analyses.
4. Combine results into the master longitudinal data set, summarize results and output data.

### Update Rhode Island meta-data

The use of higher-level functions included in the `SGP` package (e.g. `analyzeSGP`) requires the availability of state specific assessment information.  This meta-data is compiled in a `R` object named `SGPstateData` that is housed in the package.  Given the transition from the PARCC assessment program, significant updates to Rhode Island's metadata were required for the 2018 analyses.  Information was required from both PARCC and CMAS assessments, including knot and boundary values, proficiency level cutscores and other assessment program information used to configure analyses and facilitate student- and group-level reporting.

<div class='caption'>**Knots and boundaries**</div>
Cubic B-spline basis functions are used in the calculation of SGPs to more adequately model the heteroscedasticity and non-linearity found in assessment data.^[It should be noted that the independent estimation of the regression functions can potentially result in the crossing of the quantile functions.  This occurs near the extremes of the distributions and is more likely to occur given the use of non-linear functions.  A potential result of allowing the quantile functions to cross would be *lower* percentile estimations of growth for *higher* observed scale scores at the extremes (give all else equal in prior scores) and vice versa.  In order to deal with these contradictory estimates, quantile regression results are isotonized to prevent quantile crossing following the methods derived by Chernozhukov, Fernandez-Val and Glichon [-@chernozhukov2010quantile].]  These functions require the selection of boundary and interior knots.  Boundary knots (i.e. "boundaries") are end-points outside of the scale score distribution that anchor the B-spline basis.  These are typically selected by extending the entire range of scale scores by 10%.  That is, they are defined as lying 10% below the lowest obtainable/observed scale score (LOSS) and 10% above the highest obtainable/observed scale score (HOSS).  The interior knots (i.e. "knots") are the *internal* breakpoints that define the spline.  The default choice in the `SGP` package is to select the 20<sup>th</sup>, 40<sup>th</sup>, 60<sup>th</sup> and 80<sup>th</sup> quantiles of the observed scale score distribution.

In general the knots and boundaries are computed from a distribution comprised of several years of test data (i.e. multiple cohorts combined) so that any irregularities in a single year are smoothed out.  This is important because subsequent annual analyses use these same knots and boundaries as well.  All defaults were used to compile the knots and boundaries for Rhode Island from the PARCC tests in previous years, and were also used in 2018 to compute the RICAS knots and boundaries required for the calculation of the SGP standard errors.  Official knots and boundaries will be recalculated for Rhode Island RICAS assessments in 2018 when two years of test data are available and at which point they will be used as the dependent variables in *all* quantile regressions.

<div class='caption'>**Achievement level cutscores**</div>
Cutscores, which are set externally through standard-setting processes (previously by PARCC and currently by RIDE), are mainly required for student growth projections and goodness of fit plots which use prior achievement level information.  Both PARCC and RICAS cutscores and other achievement level metadata (such as labels and descriptions) were also updated to reflect the new RICAS standards.

<div class='caption'>**Conditional standard errors of measurement (CSEMs)**</div>
The calculation of SIMEX adjusted SGPs and SGP standard errors require the availability of each assessments' standard errors of measurement.  The RICAS and PARCC CSEM data for all content areas have been compiled and stored in the `SGPstateData` file.  These values may require annual updating if they change in subsequent years.


### Create SGP configurations

Unlike RICAS analyses, PSAT/SAT analyses are specialized enough so that it is necessary to specify the analyses to be performed via explicit configuration code.  Such configurations were used to conduct PSAT/SAT SGP analyses for Rhode Island.

Configurations are `R` code scripts that are used as part of the larger SGP analysis to be discussed later.  They are broken up into separate R scripts based on content domain (ELA and Mathematics).  Each configuration code chunk specifies a set of parameters that defines the norm group of students to be examined.  Every potential norm group is defined by, at a minimum, the progressions of content area, academic year and grade-level.  Therefore, every configuration must contain the first three elements listed below.  The PSAT/SAT analyses may also contain some or all of the fourth through seventh elements:

- **`sgp.content.areas`:** The progression of content areas to be looked at and their order.
- **`sgp.panel.years`:** The progression of the years associated with the content area progression (`sgp.content.areas`), potentially allowing for skipped or repeated years, etc.
- **`sgp.grade.sequences`:** The grade progression associated with the configuration content areas and years. The value **'EOCT'** stands for 'End Of Course Test'.  The use of the generic 'EOCT' allows for secondary students to be compared based on the pattern of course taking rather than being dependent upon grade-level designation.
- **`sgp.exact.grade.progression`:** When set to `TRUE`, this element will force the lower level functions to analyze *only* the progression as specified in its entirety.  Otherwise these functions will analyze subsets of the progression for every possible order (i.e. each number of prior time periods of data available). When set to `TRUE`, a norm group preference system is usually required as well.
- **`sgp.norm.group.preference`:** Because a student can potentially be included in more than one analysis/configuration, multiple SGPs will be produced for some students and a system is required to identify the preferred SGP that will be matched with the student in the `combineSGP` step.  This argument provides a ranking that specifies how preferable SGPs produced from the analysis in question is relative to other possible SGPs.  ***Lower numbers correspond with higher preference.***  Higher preference is typically given to:
    * Progressions with the greatest number of prior scale scores.
    * Progressions in which a student has repeated a course.
    * Progressions that do not include a skipped year (i.e. a gap in the scale score history).
- **`sgp.projection.grade.sequences`:** This element is used to identify the grade sequence that will be used to produce straight and/or lagged student growth projections.  It can either be left out or set explicitly to `NULL` to produce projections based on the values provided in the `sgp.content.areas` and `sgp.grade.sequences` elements.  Alternatively, when set to "`NO_PROJECTIONS`", no projections will be produced. For PSAT/SAT analyses, only configurations that correspond to the canonical course progressions can produce student growth projections. Canonical progressions are codified in the `SGP` package here: [`SGPstateData[["RI"]][["SGP_Configuration"]][["content_area.projection.sequence"]]`](https://github.com/CenterForAssessment/SGPstateData/blob/fda10b20d92f5c9dcba3fa6eaf98844bab5bb05b/SGPstateData.R#L7568).
- **`sgp.exclude.sequences`:** Lookup table containing the grade, subject, and year combinations of students that should be excluded from a cohort.  This element is used in progressions in which a year or similar time period is skipped (i.e. a gap in time exists).  For example, in a progression that goes from 8<sup>th</sup> grade Mathematics to PSAT 10 Mathematics with a skipped year in between one may want to exclude kids that repeated either 8<sup>th</sup> grade Mathematics or took another math related subject (e.g. Algebra I) in the skipped year.  Students with different course progressions may be inappropriate to include with the cohort of students who truly had no mathematics related course in the intervening year(s).  No "skipped year" analyses were included in the 2018 analyses, and so this element was not used in any of the configurations.

As an example, here is one PSAT 10 Mathematics configuration used to define a 2018 SGP analysis:

```R
...

 MATHEMATICS_PSAT_10.2017_2018 = list(
  sgp.content.areas=c("MATHEMATICS", "ALGEBRA_I", 
                      "MATHEMATICS_PSAT_10"),
  sgp.panel.years=c("2015_2016", "2016_2017", "2017_2018"),
  sgp.grade.sequences=list(c("8", "EOCT", "EOCT")),
  sgp.exact.grade.progression=TRUE,
  sgp.norm.group.preference=0,
  sgp.projection.grade.sequences=list("NO_PROJECTIONS")),
...

```

```{r, cache=FALSE, results="asis", echo=FALSE}
	Literasee:::pageBreak()
```

### Conduct SGP analyses

Due to differences in the time-frames in which the RICAS and PSAT/SAT were validated and made available, RICAS and PSAT/SAT SGPs were calculated at separate times.^[In addition to cohort-referenced (uncorrected) SGPs, SIMEX corrected SGPs were calculated and provided to RIDE for exploratory purposes.]  We first used the [`abcSGP`](https://www.rdocumentation.org/packages/SGP/versions/1.8-0.0/topics/abcSGP) function to ***a)*** do the final preparation of the 2018 PSAT/SAT formatted data and create a new `SGP` class object in the ([`prepareSGP`](https://www.rdocumentation.org/packages/SGP/versions/1.8-0.0/topics/prepareSGP) step) and ***b)*** calculate SGP estimates in the ([`analyzeSGP`](https://www.rdocumentation.org/packages/SGP/versions/1.8-0.0/topics/analyzeSGP) step).  After the RICAS data was received, cleaned and formatted as needed, the [`updateSGP`](https://www.rdocumentation.org/packages/SGP/versions/1.8-0.0/topics/updateSGP) function was used to conduct the `prepareSGP`, `analyzeSGP`, [`combineSGP`](https://www.rdocumentation.org/packages/SGP/versions/1.8-0.0/topics/combineSGP), [`outputSGP`](https://www.rdocumentation.org/packages/SGP/versions/1.8-0.0/topics/outputSGP) and [`summarizeSGP`](https://www.rdocumentation.org/packages/SGP/versions/1.8-0.0/topics/summarizeSGP) steps).  

The `combineSGP` step merges the results into the master longitudinal data set once all analyses are completed.  A pipe delimited version of the complete long data is saved in the `outputSGP` step.  The `summarizeSGP` function is used to produce many tables of descriptive statistics that are disaggregated at the state, district and school levels, as well as other factors of interest.  Finally, visualizations (such as bubble charts) are produced from the data and summary tables using the [`visualizeSGP`](https://www.rdocumentation.org/packages/SGP/versions/1.8-0.0/topics/visualizeSGP) function.

```{r, cache=FALSE, results="asis", echo=FALSE}
	Literasee:::pageBreak()
```


# Goodness of Fit

Assessment data are generally imperfect and require sophisticated statistical methods to deal with the various issues they present.  Cubic B-spline basis functions are used in the calculation of SGPs to more adequately model the heteroscedasticity, non-linearity and skewness.  Despite this, assumptions that are made in the statistical modeling process can impact how well the percentile curves fit the data.^[Independent estimation of the regression functions can potentially result in the crossing of the quantile functions.  This occurs near the extremes of the distributions and is more likely to occur given the use of non-linear functions.  A potential result of allowing the quantile functions to cross would be *lower* estimated growth percentiles for *higher* observed scale scores at the extremes (give all else equal in prior scores) and vice versa. Quantile regression results are isotonized to prevent these contradictory estimates from quantile crossing following the methods derived by Chernozhukov, Fernandez-Val and Glichon [-@chernozhukov2010quantile].]  Accordingly a thorough evaluation of the models' fit is always required.

Examination of the Rhode Island Student Growth Model goodness-of-fit was conducted by first inspecting model fit plots the `SGP` software package produced for each analysis, and subsequently inspecting student level correlations between growth and achievement.  Discussion of the model fit plots in general and examples of them are provided below, as are tables of the correlation results.  The complete portfolio of model fit plots is provided in Appendix A.

## Model Fit Plots

Using all available PARCC, RICAS and PSAT/SAT scores as the variables, estimation of student growth percentiles was conducted for each possible student (those with a current score and at least one prior score).  Each analysis is defined by the grade and content area for the grade-level analyses and exact content area (and grade when relevant) sequences for the PSAT/SAT subjects.  A goodness of fit plot is produced for each unique analysis run in 2018 and are all provided in Appendix A to this report.

As an example, Figure `r getCounter("figure")+1` shows the results for 5<sup>th</sup> grade ELA as an example of good model fit.  Figure `r getCounter("figure")+2` is an example of minor model misfit from the PSAT 10 Mathematics analysis that uses Geometry as the most recent prior.

The "Ceiling/Floor Effects Test" panel is intended to help identify potential problems in SGP estimation at the Highest and Lowest Obtainable (or Observed) Scale Scores (HOSS and LOSS).  Most often these effects are caused when it is relatively typical for extremely high (low) achieving students to consistently score at or near the HOSS (LOSS) each year leading to the SGPs for these students to be unexpectedly low (high).  That is, for example, if a sufficient number of students maintain performance at the HOSS over time, this performance will be estimated as typical, and therefore SGP estimates will reflect typical growth (e.g. 50th percentile).  In some cases small deviations from these extreme score values might even yield low growth estimates.  Although these score patterns can legitimately be estimated as a typical or low percentile, it is potentially an unfair description of actual student growth (and by proxy teacher or school, etc. performance metrics that use them).  Ultimately this is usually an artifact of the assessments' inability to adequately measure student performance at extreme ability levels.  

The table of values here shows whether the current year scale scores at both extremes yield the expected SGPs^[Note that the prior year scale scores are not represented here, but are also a critical factor in ceiling effects.].  The expectation is that the majority of SGPs for students scoring at or near the LOSS will be low (preferably less than 5 and not higher than 10), and that SGPs for students scoring at or near the HOSS will be high (preferably higher than 95 and not less than 90).  Because few students may score *exactly* at the HOSS/LOSS, the top/bottom 50 students are selected and any student scoring within their range of scores are selected for inclusion in these tables.  Consequently, there may be a range of scores at the HOSS/LOSS rather than a single score, and there may be more than 50 students included in the HOSS/LOSS row if the 50 students at the extremes only contain the single HOSS/LOSS score.

This table is meant to serve more as a "canary in the coal mine" than as a detailed, conclusive indicator of ceiling or floor effects, and a more fine grained analysis that considers the relationship between score histories and SGPs may be necessary.  Appendix C of this report provides a more in depth investigation.

The two bottom panels compare the observed conditional density of the SGP estimates with the theoretical (uniform) density.  The bottom left panel shows the empirical distribution of SGPs given prior scale score deciles in the form of a 10 by 10 cell grid.  Percentages of student growth percentiles between the 10<sup>th</sup>, 20<sup>th</sup>, 30<sup>th</sup>, 40<sup>th</sup>, 50<sup>th</sup>, 60<sup>th</sup>, 70<sup>th</sup>, 80<sup>th</sup>, and 90<sup>th</sup> percentiles were calculated based upon the empirical decile of the cohort's prior year scaled score distribution^[The total students in each analysis varies depending on grade and subject, and prior score deciles are based only on scores for students used in the SGP calculations.].  With an infinite population of test takers, at each prior scaled score, with perfect model fit, the expectation is to have 10 percent of the estimated growth percentiles between 1 and 9, 10 and 19, 20 and 29, ..., and 90 and 99.  Deviations from 10 percent, indicated by red and blue shading, suggests lack of model fit.  The further *above* 10 the darker the red, and the further *below* 10 the darker the blue.  

When large deviations occur, one likely cause is a clustering of scale scores that makes it impossible to "split" the score at a dividing point forcing a majority of the scores into an adjacent cell.  This occurs more often in lowest grade levels where fewer prior scores are available (particularly in the lowest grade when only a single prior is available).  Another common cause of this is small cohort size (e.g. fewer than 5,000 students).

The bottom right panel of each plot is a Q-Q plot which compares the observed distribution of SGPs with the theoretical (uniform) distribution.  An ideal plot here will show black step function lines that do not deviate greatly from the ideal, red line which traces the 45 degree angle of perfect fit.

```{r, cache=TRUE, echo=FALSE, include=FALSE, GOFplots}
  ##    Create goodness of fit plots for tech report example
	dir.create("../img", recursive=TRUE, showWarnings=FALSE)
  setwd("../img")

  setkeyv(Rhode_Island_SGP@Data, SGP:::getKey(Rhode_Island_SGP))

	### ELA Grade 5 as example of GOOD fit ...
  ##  Choice of 5th grade and ELA are arbitrary ...
  ##  Keep using that from year to year to show it IS arbitrary - no need to change year to year ... ...

  dat <- Rhode_Island_SGP@Data[grepl("2016_2017/ELA_4; 2017_2018/ELA_5", SGP_NORM_GROUP), #SGP_NORM_GROUP %in% "2015_2016/ELA_6; ", 
  				 c("VALID_CASE", "CONTENT_AREA", "GRADE", "YEAR", "ID", "SGP", "SCALE_SCORE", "SCALE_SCORE_PRIOR", "SGP_NORM_GROUP"), with=FALSE]
	gofSGP(dat, state="RI", years='2017_2018', content_areas="ELA", use.sgp="SGP", output.format="PNG")

  dat <- Rhode_Island_SGP@Data[ grepl("2016_2017/GEOMETRY_EOCT; 2017_2018/MATHEMATICS_PSAT_10_EOCT", SGP_NORM_GROUP),
  				 c("VALID_CASE", "CONTENT_AREA", "GRADE", "YEAR", "ID", "SGP", "SCALE_SCORE", "SCALE_SCORE_PRIOR", "SGP_NORM_GROUP"), with=FALSE]
	gofSGP(dat, state="RI", years='2017_2018', content_areas='MATHEMATICS_PSAT_10', use.sgp="SGP", output.format="PNG")

	setwd("../2018")
```


###  Examples of model fit

We provide two examples here of cohort-referenced model fit.  Overall the 2018 RICAS and PSAT/SAT results in all subjects are excellent with few exceptions.  See Appendix A for all goodness of fit plots.

Figure `r getCounter("figure")+1` shows the results for 5<sup>th</sup> grade ELA as an example of good model fit. 

```{r, results="asis", echo=FALSE, G5_ELA_GoF_Cohort}
		placeFigure(page.break= TRUE,
			files = "../img/Goodness_of_Fit/ELA.2017_2018/gofSGP_Grade_5.png",
			caption = "Goodness of Fit Plot for 2018 5<sup>th</sup> Grade ELA:  Example of good model fit.")
```

Minor misfit in the PSAT 10 Mathematics model is likely due to the relatively small cohort size (2,000+ kids).  This cohort is a relatively homogenous cohort of high academic achievers compared to others (90% at Level 3 or above in Geometry).  The indication of a potential ceiling effect in this plot is probably overstated given the range of scores included in the HOSS (bottom) row.  That is, the low SGPs are likely attributed to students scoring just below the HOSS, and therefore less concerning.  This situation is investigated in greater detail in Appendix C of this report.

```{r, results="asis", echo=FALSE, PSAT10M_GoF_Cohort}
		placeFigure(page.break= TRUE,
			files = "../img/Goodness_of_Fit/MATHEMATICS_PSAT_10.2017_2018/2017_2018_MATH_PSAT_10_EOCT;2016_2017_GEOMETRY_EOCT.png",
			caption = "Goodness of Fit Plot for a 2018 Geometry Progression: Example of slight model mis-fit.")
```


## Growth and Prior Achievement at the Student Level

To investigate the possibility that individual level misfit might impact summary level results, student level SGP results were examined relative to prior achievement.  With perfect fit to data, the correlation between students' most recent prior achievement scores and their student growth percentiles is zero (i.e., the goodness of fit tables would have a uniform distribution of percentiles across all previous scale score levels).  To investigate in another way, correlations between **a)** prior and current scale scores (achievement) and **b)** prior score and student growth percentiles were calculated.  Evidence of good model fit begins with a strong positive relationship between prior and current achievement, which suggests that growth is detectable and modeling it is reasonable to begin with.  A lack of relationship (zero correlation) between prior achievement and growth confirms that the model has fit the data well and produced a uniform distribution of percentiles across all previous scale score levels.

Student-level correlations for grade-level RICAS subjects are presented in Table `r tblNum(1)`, and the results are generally as expected.  Strong relationships exist between prior and current scale scores for the grade level analyses (column 3).  The correlation between students' most recent prior achievement scores and their student growth percentiles is zero when the model is perfectly fit to the data.  This also indicates that students can demonstrate high (or low) growth regardless of prior achievement.


### Grade-level RICAS subjects

```{r, results='asis', echo=FALSE, Student_EOG}
	student.cor.grd <- Rhode_Island_SGP@Data[YEAR=='2017_2018' & VALID_CASE=='VALID_CASE' & CONTENT_AREA %in% GL_subjects][, list(
		`$\\\rr_ { Test Scores}$` = round(cor(SCALE_SCORE, SCALE_SCORE_PRIOR_STANDARDIZED, use='pairwise.complete'), 2), 
		`$\\\rr_ { SGP}$` = round(cor(SGP, SCALE_SCORE_PRIOR_STANDARDIZED, use='pairwise.complete'), 2), 
		N_Size = sum(!is.na(SGP))), keyby = list(CONTENT_AREA, GRADE)]
	
	gl_tmp_tbl <- student.cor.grd[!is.na(student.cor.grd[["$\\\rr_ { SGP}$"]])]
	gl_tmp_tbl[, GRADE := as.numeric(GRADE)]
	setkey(gl_tmp_tbl, CONTENT_AREA, GRADE)
	gl_tmp_tbl <- gl_tmp_tbl[][order(match(gl_tmp_tbl$CONTENT_AREA, GL_subjects))]

	tmp.cap <- "RICAS Student Level Correlations between Prior Standardized Scale Score and 1) Current Scale Score and 2) SGP."
	gl_tmp_tbl$CONTENT_AREA <- sapply(gl_tmp_tbl$CONTENT_AREA, capwords, USE.NAMES=FALSE)
	gl_tmp_tbl$CONTENT_AREA[duplicated(gl_tmp_tbl$CONTENT_AREA)] <- ""
	gl_tmp_tbl$N_Size <- prettyNum(gl_tmp_tbl$N_Size, preserve.width = "individual", big.mark=',')
	setnames(gl_tmp_tbl, c(1:2,5), sapply(names(gl_tmp_tbl)[c(1:2,5)], capwords))

  cat(dualTable(as.matrix(gl_tmp_tbl), align=paste(rep('r', dim(gl_tmp_tbl)[2]), collapse=''), caption = tmp.cap))

  Literasee:::pageBreak()
```

### PSAT/SAT Subjects

PSAT/SAT test subjects may be analyzed using more than one sequence of prior subjects, grades and years, and these unique progressions are disaggregated in Table `r tblNum(1)` using the most recent prior available for each norm group (although more prior years' scores are used in SGP calculations when available).  The correlations between current and prior scale score here are notably lower than in the RICAS norm groups.  Lower correlations may be expected in PSAT/SAT subjects due to lack of instruction in test content (e.g. students who have not yet received instruction in Geometry are tested on Geometry related items) or other issues of misalignment of the test content and Rhode Island's standards and curriculum.


```{r, cache=TRUE, echo=FALSE, include=FALSE, Student_EOCT}
	student.cor.eoct <- Rhode_Island_SGP@Data[YEAR=='2017_2018' & VALID_CASE=='VALID_CASE' & CONTENT_AREA %in% EOCT_subjects][, list(
		`$\\\rr_ {  Test Scores}$` = round(cor(SCALE_SCORE, SCALE_SCORE_PRIOR_STANDARDIZED, use='pairwise.complete'), 2), 
		`$\\\rr_ {  SGP}$` = round(cor(SGP, SCALE_SCORE_PRIOR_STANDARDIZED, use='pairwise.complete'), 2), 
		N_Size = sum(!is.na(SGP))), keyby = list(CONTENT_AREA, Most_Recent_Prior)]
```
```{r, results='asis', echo=FALSE, Student_EOCT_1}
	eoct_tmp_tbl <- student.cor.eoct[!is.na(student.cor.eoct[["$\\\rr_ {  SGP}$"]])]

	eoct_tmp_tbl <- eoct_tmp_tbl[][order(match(eoct_tmp_tbl$CONTENT_AREA, EOCT_subjects))]
	eoct_tmp_tbl$CONTENT_AREA <- sapply(eoct_tmp_tbl$CONTENT_AREA, capwords, USE.NAMES=FALSE)

	tmp.cap <- "PSAT/SAT Student Level Correlations between Prior Standardized Scale Score and 1) Current Scale Score and 2) SGP - Disaggregated by Norm Group."

	eoct_tmp_tbl[, Most_Recent_Prior := sapply(gsub("/", " ", Most_Recent_Prior), capwords)]
	eoct_tmp_tbl[, Most_Recent_Prior := gsub(" Eoct", "", Most_Recent_Prior)]
	eoct_tmp_tbl[, Most_Recent_Prior := gsub(" 8", " Grade 8", Most_Recent_Prior)]
	eoct_tmp_tbl[, Most_Recent_Prior := gsub(" 7", " Grade 7", Most_Recent_Prior)]
	eoct_tmp_tbl[, Most_Recent_Prior := gsub("Mathematics", "Math", Most_Recent_Prior)]
	eoct_tmp_tbl[, Most_Recent_Prior := gsub("2016 2017", "2016-2017", Most_Recent_Prior)]

	eoct_tmp_tbl$CONTENT_AREA <- sapply(eoct_tmp_tbl$CONTENT_AREA, capwords, USE.NAMES=FALSE)
	eoct_tmp_tbl$CONTENT_AREA[duplicated(eoct_tmp_tbl$CONTENT_AREA)] <- ""
	eoct_tmp_tbl$N_Size <- prettyNum(eoct_tmp_tbl$N_Size, preserve.width = "individual", big.mark=',')
	setnames(eoct_tmp_tbl, c(1:2,5), sapply(names(eoct_tmp_tbl)[c(1:2,5)], capwords))

	cat(dualTable(as.matrix(eoct_tmp_tbl), align=paste(rep('r', dim(eoct_tmp_tbl)[2]), collapse=''), caption = tmp.cap))
	
	Literasee:::pageBreak()
```


# SGP Results

Growth percentiles, being quantities associated with each individual student, can be easily summarized across numerous grouping indicators to provide summary results regarding growth.  The median and mean of a collection of growth percentiles are used as measures of central tendency that summarize the distribution as a single number.  With perfect data fit, we expect the state median of all student growth percentiles in any grade to be 50 because the data are norm-referenced across all students in the state.  Median (and mean) growth percentiles well below 50 represent growth less than the state "average" and median growth percentiles well above 50 represent growth in excess of the state "average".

To demonstrate the norm-referenced nature of the growth percentiles viewed at the state level, Tables `r tblNum(1)` and  `r tblNum(2)` present the cohort-referenced growth percentile medians and means for the RICAS and PSAT/SAT content areas respectively.

```{r, results='asis', echo=FALSE, SumEOG}
	EOG_smry <- Rhode_Island_SGP@Data[CONTENT_AREA %in% GL_subjects & YEAR=='2017_2018'][, list(MEDIAN = median(as.numeric(SGP), na.rm=TRUE), MEAN = round(mean(SGP, na.rm=TRUE), 1)), by=c('CONTENT_AREA', 'GRADE')][!is.na(MEDIAN)]
	setkey(EOG_smry)

	EOG_smryB <- data.frame()
	for (ca in GL_subjects){
		tmp_EOG_smry <- paste(t(as.matrix(EOG_smry[CONTENT_AREA==ca,][, list(MEDIAN)])), " (", t(as.matrix(EOG_smry[CONTENT_AREA==ca,][, list(MEAN)])), ")", sep="")
		tmp_EOG_smry <- data.frame(matrix(c(capwords(ca), tmp_EOG_smry), 1, length(tmp_EOG_smry)+1))
		names(tmp_EOG_smry) <- c("Content Area", t(as.matrix(EOG_smry[CONTENT_AREA==ca,][, list(GRADE)])))
		EOG_smryB <- rbindlist(list(EOG_smryB, tmp_EOG_smry))
	}
	EOG_smryB[is.na(EOG_smryB)] <- ""
	
  cat(dualTable(as.matrix(EOG_smryB), title="", n.cgroup=c(1, dim(EOG_smryB)[2]-1), cgroup=c("", "Grades"),
  	align=paste(rep('r', dim(EOG_smryB)[2]), collapse=''),
		caption='2018 RICAS Median (Mean) Student Growth Percentile by Grade and Content Area.'))
```

```{r, results='asis', echo=FALSE, SumEOC}
	EOCT_smry <- Rhode_Island_SGP@Data[CONTENT_AREA %in% EOCT_subjects & YEAR=='2017_2018'][,list("Median SGP"=median(as.numeric(SGP), na.rm=TRUE), "Mean SGP"=round(mean(SGP, na.rm=TRUE), 1)), by='CONTENT_AREA']
	setkey(EOCT_smry)

	EOCT_smry_B <- EOCT_smry[match(EOCT_subjects, EOCT_smry[["CONTENT_AREA"]]) ,]
	EOCT_smry_B <- EOCT_smry_B[, CONTENT_AREA := sapply(CONTENT_AREA, capwords, special.words=c("PSAT", "SAT"))] #  need to use <- assignment with := to avoid print out of DT...
	setnames(EOCT_smry_B, "CONTENT_AREA", "Content Area")
	
  cat(dualTable(as.matrix(EOCT_smry_B), align='rcc',
		caption='2018 PSAT/SAT Median and Mean Student Growth Percentile by Content Area.'))
```

Based upon perfect model fit to the data, the median of all state growth percentiles in each grade by year by subject combination should be 50.  That is, in the conditional distributions, 50 percent of growth percentiles should be less than 50 and 50 percent should be greater than 50.  Deviations from 50 indicate imperfect model fit to the data.  Imperfect model fit can occur for a number of reasons, including issues with the data (e.g., floor and ceiling effects leading to a "bunching" up of the data) or how the SGP function fits the data.  PSAT/SAT results are further complicated by the aggregation across multiple analyses (prior course patterns).  The results in Tables `r tblNum(-1)` and  `r tblNum(0)` are close to perfect, with almost all values equal to 50.

The results are coarse in that they are aggregated across tens of thousands of students.  More refined fit analyses were presented in the Goodness-of-Fit section.  Depending upon feedback from RIDE, it may be desirable to tweak some operational parameters and attempt to improve fit even further.  The impact upon the operational results based on better fit is expected to be extremely minor.

It is important to note how, at the entire state level, the *norm-referenced* growth information returns little information on annual trends due to its norm-reference nature.  What the results indicate is that a typical (or average) student in the state demonstrates 50<sup>th</sup> percentile growth.  That is, "typical students" demonstrate "typical growth".  One benefit of the norm-referenced results follows when subgroups are examined (e.g., schools, district, demographic groups, etc.) Examining subgroups in terms of the mean or median of their student growth percentiles, it is then possible to investigate why some subgroups display lower/higher student growth than others.  Moreover, because the subgroup summary statistic (i.e., the median) is composed of many individual student growth percentiles, one can break out the result and further examine the distribution of individual results.  


## Group Level Results

Unlike reporting SGPs at the individual level, when aggregating to the group level (e.g., school) the correlation between aggregate prior student achievement and aggregate growth is rarely zero. The correlation between prior student achievement and growth at the school level is a compelling descriptive statistic because it indicates whether students attending schools serving higher achieving students grow faster (on average) than those students attending schools serving lower achieving students. Results from previous state analyses show a correlation between prior achievement of students associated with a current school (quantified as percent at/above proficient) and the median SGP are typically between 0.1 and 0.3 (although higher numbers have been observed in some states as well). That is, these results indicate that on average, students attending schools serving lower achieving students tend to demonstrate less exemplary growth than those attending schools serving higher achieving students. Equivalently, based upon ordinary least squares (OLS) regression assumptions, the prior achievement level of students attending a school accounts for between 1 and 10 percent of the variability observed in student growth. There are no definitive numbers on what this correlation should be, but recent studies on value-added models show similar results [@MccaLock:2008].

### School Level Results

To illustrate these relationships visually, the bubble charts in Figures `r getCounter("figure")+1` and `r getCounter("figure")+2` depict growth as quantified by the median SGP of students at the school against prior achievement status, quantified by percentage of student at/above proficient at the school.  "Prior Percent at/above Proficient" in this case is determined by the percent of student's that scored in the "Level 4" or "Level 5" range of the prior year's PARCC test out of all student's that received a score.  The charts have been successful in helping to motivate the discussion of the two qualities: student achievement and student growth.  Though the figures are not detailed enough to indicate strength of relationship between growth and achievement, they are suggestive and valuable for discussions with stakeholders who are being introduced to the growth model for the first time. 

```{r, cache=TRUE, echo=FALSE, include=FALSE, BubblePlots}
	#  Needed to add in Prior Achievement for 2018
	visualizeSGP(Rhode_Island_SGP,
 		plot.types = "bubblePlot",
 		bPlot.years=  "2017_2018",
 		# bPlot.content_areas=GL_subjects,
 		bPlot.anonymize=TRUE,
 		bPlot.folder = "../img/Bubble_Plots",
 		bPlot.output = "PNG")

	old.names <- list.files("../img/Bubble_Plots/2017_2018/State/Style_1", full.names = TRUE)
	new.names <- gsub("Rhode Island", "Rhode_Island", old.names)
	file.rename(old.names, new.names)
```

```{r, results="asis", echo=FALSE, ELA_Bubble}
		placeFigure(
			files = "../img/Bubble_Plots/2017_2018/State/Style_1/Rhode_Island_2017_2018_ELA_State_Bubble_Plot_(Prior_Achievement).png",
			caption = "School-level Bubble Plots for Rhode Island: ELA, 2017-2018.", pdf.width = 0.925)
```


```{r, results="asis", echo=FALSE, Math_Bubble}
		placeFigure(
			files = "../img/Bubble_Plots/2017_2018/State/Style_1/Rhode_Island_2017_2018_Mathematics_State_Bubble_Plot_(Prior_Achievement).png",
			caption = "School-level Bubble Plots for Rhode Island: Mathematics, 2017-2018.", pdf.width = 0.925)
```

```{r, results="asis", echo=FALSE, PSAT10_ELA_Bubble}
		placeFigure(
			files = "../img/Bubble_Plots/2017_2018/State/Style_1/Rhode_Island_2017_2018_ELA_PSAT_10_State_Bubble_Plot_(Prior_Achievement).png",
			caption = "School-level Bubble Plots for Rhode Island: PSAT 10 ELA, 2017-2018.", pdf.width = 0.925)
```

```{r, results="asis", echo=FALSE, SAT_ELA_Bubble}
		placeFigure(
			files = "../img/Bubble_Plots/2017_2018/State/Style_1/Rhode_Island_2017_2018_ELA_SAT_State_Bubble_Plot_(Prior_Achievement).png",
			caption = "School-level Bubble Plots for Rhode Island: SAT ELA, 2017-2018.", pdf.width = 0.925)
```

```{r, results="asis", echo=FALSE, PSAT10_Math_Bubble}
		placeFigure(
			files = "../img/Bubble_Plots/2017_2018/State/Style_1/Rhode_Island_2017_2018_Mathematics_PSAT_10_State_Bubble_Plot_(Prior_Achievement).png",
			caption = "School-level Bubble Plots for Rhode Island: PSAT 10 Math, 2017-2018.", pdf.width = 0.925)
```

```{r, results="asis", echo=FALSE, SAT_Math_Bubble}
		placeFigure(
			files = "../img/Bubble_Plots/2017_2018/State/Style_1/Rhode_Island_2017_2018_Mathematics_SAT_State_Bubble_Plot_(Prior_Achievement).png",
			caption = "School-level Bubble Plots for Rhode Island: SAT Math, 2017-2018.", pdf.width = 0.925)
```

The relationship between average prior student achievement and median SGP observed for Rhode Island is relatively strong compared to some other states for which the NCIEA has done SGP analyses.  Table `r tblNum(1)` shows overall correlation between prior achievement (measured here as the mean prior standardized scale score) for the previous three years.  All results shown here are for schools with 10 or more students.


```{r, cache=TRUE, echo=FALSE, include=FALSE}
	sch.msgp <- Rhode_Island_SGP@Summary$SCHOOL_NUMBER$SCHOOL_NUMBER__YEAR__SCHOOL_ENROLLMENT_STATUS[YEAR %in% c('2015_2016', '2016_2017', '2017_2018') & !is.na(SCHOOL_NUMBER) & MEDIAN_SGP_COUNT > 9] # Last 3 YEARs
	sch.msgp.subj <- Rhode_Island_SGP@Summary$SCHOOL_NUMBER$SCHOOL_NUMBER__CONTENT_AREA__YEAR__SCHOOL_ENROLLMENT_STATUS[YEAR %in% c('2015_2016', '2016_2017', '2017_2018') & !is.na(SCHOOL_NUMBER) & MEDIAN_SGP_COUNT > 9] # Last 3 YEARs
	sch.msgp.grd <- Rhode_Island_SGP@Summary$SCHOOL_NUMBER$SCHOOL_NUMBER__CONTENT_AREA__YEAR__GRADE__SCHOOL_ENROLLMENT_STATUS[YEAR == '2017_2018' & !is.na(SCHOOL_NUMBER) & MEDIAN_SGP_COUNT > 9]
	sch.msgp.grd$CONTENT_AREA <- ordered(sch.msgp.grd$CONTENT_AREA, levels = c(GL_subjects, EOCT_subjects)) # Last 3 YEARs

	##  Combined subjects - School_Cor_Grand
	sch.cor <- sch.msgp[, list(
			MEDIAN_SGP = round(cor(MEDIAN_SGP, MEAN_SCALE_SCORE_PRIOR_STANDARDIZED), 2),
			MEAN_SGP=round(cor(MEAN_SGP, MEAN_SCALE_SCORE_PRIOR_STANDARDIZED), 2)), keyby = 'YEAR']

	##  Multiple year - School_EOG_Correlations, School_EOCT_Cor
	sch.cor.subj <- sch.msgp.subj[, list(
			MEDIAN_SGP = round(cor(MEDIAN_SGP, MEAN_SCALE_SCORE_PRIOR_STANDARDIZED), 2),
			MEAN_SGP = round(cor(MEAN_SGP, MEAN_SCALE_SCORE_PRIOR_STANDARDIZED), 2)), keyby = c('CONTENT_AREA', 'YEAR')]

	## 2018 by grade - School_Grade_EOG_Correlations
	sch.cor.grd <- sch.msgp.grd[, list(
		MEDIAN_SGP = round(cor(MEDIAN_SGP, MEAN_SCALE_SCORE_PRIOR_STANDARDIZED), 2),
		MEAN_SGP = round(cor(MEAN_SGP, MEAN_SCALE_SCORE_PRIOR_STANDARDIZED), 2)), keyby = c('CONTENT_AREA', 'GRADE')]
	sch.cor.grd <- sch.cor.grd[order(match(sch.cor.grd$CONTENT_AREA, c(GL_subjects, EOCT_subjects))),]
```

```{r, results='asis', echo=FALSE, School_Cor_Grand}
	tmp_tbl <- data.frame(sch.cor)
	###  Number of "column groups" - here different aggregates correlated with mean standardized prior score
	# n_col_group <- c(1,4)  # Just aggregate SGP groups.
	# col_group <- c("", "SGP Aggregate Type")
	###  Number of "row groups" - none here  
	n_row_group <- NULL
	row_group <- NULL
	tmp_capt <- "Correlations between Mean Prior Standardized Scale Score and Aggregate SGPs - (Combined Subjects)"
	names(tmp_tbl) <- sapply(names(tmp_tbl), capwords)
	tmp_tbl[is.na(tmp_tbl)] <- ""
	tmp_tbl$Year <- gsub("_", "-", tmp_tbl$Year)
	
  cat(dualTable(as.matrix(tmp_tbl), align=paste(rep('c', dim(tmp_tbl)[2]), collapse=''), compatibility="CSS", caption= tmp_capt))

  Literasee:::pageBreak()   
```


Correlation tables describing the relationship between prior standardized scale score and aggregate growth percentiles are presented below in separate subsections for RICAS and PSAT/SAT subjects.  The first correlation table in the each subsection provides these overall SGP aggregates' relationships with mean prior standardized scale scores.  The additional correlation tables are disaggregated by content area, and content area and grade to provide more detail.


<div class='caption'>**End-of-Grade Content Areas**</div>


```{r, results='asis', echo=FALSE, School_EOG_Correlations}
	tmp_tbl <- sch.cor.subj[order(match(sch.cor.subj$CONTENT_AREA, GL_subjects)),][CONTENT_AREA %in% GL_subjects]
	tmp.cap <- "School Level RICAS Correlations between Mean Prior Standardized Scale Score and Aggregate SGPs by Content Area."	
	tmp_tbl$CONTENT_AREA <- sapply(tmp_tbl$CONTENT_AREA, capwords, USE.NAMES=FALSE)
	tmp_tbl$CONTENT_AREA[duplicated(tmp_tbl$CONTENT_AREA)] <- ""
	setnames(tmp_tbl, sapply(names(tmp_tbl), capwords))

	tmp_tbl[is.na(tmp_tbl)] <- ""
	tmp_tbl$Year <- gsub("_", "-", tmp_tbl$Year)

	cat(dualTable(as.matrix(tmp_tbl), align=c('r', 'r', rep('c', dim(tmp_tbl)[2]-2)), caption  = tmp.cap))
```
<p></p>

```{r, results='asis', echo=FALSE, School_Grade_EOG_Correlations}
	gl_cor_tbl <- data.frame(sch.cor.grd[CONTENT_AREA %in% GL_subjects])

	tmp.cap <- "2018 School Level RICAS Correlations between Mean Prior Standardized Scale Score and Aggregate SGPs by Content Area and Grade."
	gl_cor_tbl$CONTENT_AREA <- sapply(gl_cor_tbl$CONTENT_AREA, capwords)
	gl_cor_tbl$CONTENT_AREA[duplicated(gl_cor_tbl$CONTENT_AREA)] <- ""
	setnames(gl_cor_tbl, sapply(names(gl_cor_tbl), capwords))

	cat(dualTable(as.matrix(gl_cor_tbl), align=c('r', 'r', rep('c', dim(gl_cor_tbl)[2]-2)), caption = tmp.cap))
	
	Literasee:::pageBreak()
```

<div class='caption'>**PSAT/SAT Subjects**</div>


```{r, results='asis', echo=FALSE, School_EOCT_Cor}
	# sch.psat.sat <- Rhode_Island_SGP@Data[YEAR %in% c('2017_2018') & !is.na(SCHOOL_NUMBER) & CONTENT_AREA %in% EOCT_subjects & !is.na(SGP),][, list(
	# 		MEDIAN_SGP = median(SGP, na.rm=TRUE),
	# 		MEAN_SGP=mean(SGP, na.rm=TRUE),
	# 		MEAN_SCALE_SCORE_PRIOR_STANDARDIZED = mean(SCALE_SCORE_PRIOR_STANDARDIZED, na.rm=TRUE),
	# 		MEDIAN_SGP_COUNT = .N), keyby = c("SCHOOL_NUMBER", "CONTENT_AREA", 'YEAR')] # sum(!is.na(as.integer(SGP)))
	# 
	# sch.cor.subj2 <- sch.psat.sat[MEDIAN_SGP_COUNT > 9][, list(
	# 		MEDIAN_SGP = round(cor(MEDIAN_SGP, MEAN_SCALE_SCORE_PRIOR_STANDARDIZED), 2),
	# 		MEAN_SGP=round(cor(MEAN_SGP, MEAN_SCALE_SCORE_PRIOR_STANDARDIZED), 2),
	# 		N=.N), keyby = c('CONTENT_AREA', 'YEAR')]

	tmp_tbl <- sch.cor.subj[order(match(sch.cor.subj$CONTENT_AREA, EOCT_subjects)),][CONTENT_AREA %in% EOCT_subjects]
	tmp.cap <- "School Level PSAT/SAT Correlations between Mean Prior Standardized Scale Score and Aggregate SGPs by Content Area."
	tmp_tbl$CONTENT_AREA <- sapply(tmp_tbl$CONTENT_AREA, capwords, USE.NAMES=FALSE)
	tmp_tbl$CONTENT_AREA[duplicated(tmp_tbl$CONTENT_AREA)] <- ""
	setnames(tmp_tbl, sapply(names(tmp_tbl), capwords))

	tmp_tbl[is.na(tmp_tbl)] <- ""
	tmp_tbl$Year <- gsub("_", "-", tmp_tbl$Year)

	cat(dualTable(as.matrix(tmp_tbl), align=c('r', 'r', rep('c', dim(tmp_tbl)[2]-2)), caption = tmp.cap))
	
	Literasee:::pageBreak()
```


# References
